{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-chess==0.31.2\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here: Next move prediction given game sequence (PGN) and probing on legal moves given all starting squares.\n",
    "\n",
    "import chess\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, AutoModel\n",
    "sys.path.append(\"src\")\n",
    "from data_utils.chess_tokenizer import ChessTokenizer\n",
    "import chess.pgn as pgn\n",
    "import io\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize the Model and the Tokenizer\n",
    "\n",
    "vocab_path = \"vocab.txt\"\n",
    "tokenizer = ChessTokenizer(vocab_path)\n",
    "model = GPT2LMHeadModel.from_pretrained('shtoshni/gpt2-chess-uci')\n",
    "\n",
    "def get_legal_moves(board):\n",
    "    legal_moves = set()\n",
    "    for move in board.legal_moves:\n",
    "        uci_move = board.uci(move)\n",
    "        legal_moves.add(uci_move)\n",
    "    return legal_moves\n",
    "\n",
    "ds = load_dataset(\"adamkarvonen/chess_sae_individual_games_filtered\")\n",
    "\n",
    "def chop_game_at_random_point(pgn):\n",
    "    # Split the PGN string into individual moves\n",
    "    moves = pgn.split()\n",
    "    \n",
    "    # Filter out the move numbers (e.g., \"1.\", \"2.\", etc.)\n",
    "    game_moves = [move for move in moves if not move[0].isdigit()]\n",
    "    \n",
    "    # Determine the total number of moves\n",
    "    total_moves = len(game_moves)\n",
    "    \n",
    "    # Ensure we have an even number of moves for simplicity\n",
    "    if total_moves % 2 != 0:\n",
    "        total_moves -= 1\n",
    "    \n",
    "    # Randomly choose a point to chop, ensuring 50/50 white/black split\n",
    "    random_index = random.randint(7, total_moves - 1)\n",
    "    \n",
    "    # Determine if the random point should be a white or black move\n",
    "    if random_index % 2 == 0:\n",
    "        chopped_game = ' '.join(moves[:random_index + 1])\n",
    "    else:\n",
    "        chopped_game = ' '.join(moves[:random_index + 2])\n",
    "    \n",
    "    return chopped_game\n",
    "\n",
    "def chop_game_before_final_moves(pgn):\n",
    "    moves = pgn.split()\n",
    "    game_moves = [move for move in moves if not move[0].isdigit()]\n",
    "    total_moves = len(game_moves)\n",
    "    if total_moves < 2 :\n",
    "        chopped_game = ' '.join(moves)\n",
    "    else:\n",
    "        chopped_game = ' '.join(moves[:total_moves - 2])\n",
    "\n",
    "    return chopped_game\n",
    "\n",
    "\n",
    "# probing to see if the game provides legal moves given the starting square\n",
    "def predict_move1(move):\n",
    "\n",
    "    game_prefix = [tokenizer.bos_token_id]\n",
    "    game_prefix_str = \"\"\n",
    "\n",
    "    game_prefix.extend(tokenizer.encode(move, add_special_tokens=False, get_move_end_positions=False))\n",
    "    game_prefix_str += move + \" \"\n",
    "\n",
    "    greedy_game_prefix = list(game_prefix)\n",
    "    prefix_tens = torch.tensor([greedy_game_prefix])\n",
    "    pred_move = \"\"\n",
    "\n",
    "    #range was 3 in orig.\n",
    "    for idx in range(3):\n",
    "        logits = model(prefix_tens)[0]\n",
    "        \n",
    "        # logits is a 3-dimensional tensor with the shape [batch_size, sequence_length, vocab_size].\n",
    "        # --> Extracting the logits for the last token.\n",
    "        last_token_logit = logits[0, -1, :]\n",
    "\n",
    "        token_idx = torch.argmax(last_token_logit).item()\n",
    "        \n",
    "        current_token = tokenizer.decode_token(token_idx)\n",
    "        pred_move += current_token\n",
    "\n",
    "        if idx == 0 and current_token == tokenizer.eos_token:\n",
    "            break\n",
    "\n",
    "        greedy_game_prefix += [token_idx]\n",
    "        prefix_tens = torch.tensor([greedy_game_prefix])\n",
    "\n",
    "    if len(pred_move) == 6:\n",
    "        pred_move = pred_move[:4]\n",
    "\n",
    "    legal_moves = get_legal_moves(board)\n",
    "    if pred_move not in legal_moves:\n",
    "        print(\"ILLEGAL MOVE\")\n",
    "        print(f\"Legal moves: {legal_moves}\")\n",
    "\n",
    "    print(f\"LM plays: {pred_move}\")\n",
    "\n",
    "def probing_legal_moves1(move0):\n",
    "    legal_moves = get_legal_moves(board)\n",
    "    illegal_count = 0\n",
    "    for leg_move in legal_moves:\n",
    "\n",
    "        initial_square = leg_move[:2]\n",
    "        move = move0 + \" \" + initial_square\n",
    "\n",
    "        game_prefix = [tokenizer.bos_token_id]\n",
    "        game_prefix_str = \"\"\n",
    "\n",
    "        game_prefix.extend(tokenizer.encode(move, add_special_tokens=False, get_move_end_positions=False))\n",
    "        game_prefix_str += move + \" \"\n",
    "\n",
    "        greedy_game_prefix = list(game_prefix)\n",
    "        prefix_tens = torch.tensor([greedy_game_prefix])\n",
    "        pred_move = \"\"\n",
    "\n",
    "        for idx in range(1):\n",
    "            logits = model(prefix_tens)[0]\n",
    "        \n",
    "            # logits is a 3-dimensional tensor with the shape [batch_size, sequence_length, vocab_size].\n",
    "            # --> Extracting the logits for the last token.\n",
    "            last_token_logit = logits[0, -1, :]\n",
    "\n",
    "            token_idx = torch.argmax(last_token_logit).item()\n",
    "            \n",
    "            current_token = tokenizer.decode_token(token_idx)\n",
    "            pred_move += current_token\n",
    "\n",
    "            if idx == 0 and current_token == tokenizer.eos_token:\n",
    "                break\n",
    "\n",
    "            greedy_game_prefix += [token_idx]\n",
    "            prefix_tens = torch.tensor([greedy_game_prefix])\n",
    "\n",
    "            if len(pred_move) == 6:\n",
    "                pred_move = pred_move[:4]\n",
    "\n",
    "\n",
    "            pred_move = initial_square + pred_move\n",
    "            if pred_move not in legal_moves:\n",
    "            #print(\"ILLEGAL MOVE\")\n",
    "            #print(f\"Legal moves: {legal_moves}\")\n",
    "                illegal_count += 1\n",
    "\n",
    "            #print(f\"LM plays: {pred_move}\")\n",
    "\n",
    "    #print(f\"Ratio legal/total: {len(legal_moves)-illegal_count} / {len(legal_moves)}\")\n",
    "    return (len(legal_moves)-illegal_count), len(legal_moves)\n",
    "\n",
    "\n",
    "g = random.randint(0, 90000)\n",
    "a = ds['train']['text'][g][1:]\n",
    "b = chop_game_at_random_point(a)\n",
    "\n",
    "b1 = io.StringIO(b)\n",
    "game = pgn.read_game(b1)\n",
    "board = game.board()\n",
    "\n",
    "move_list = []\n",
    "for move in game.mainline_moves():\n",
    "    move_list.append(board.uci(move))\n",
    "    board.push(move)\n",
    "b1 = (\" \".join(move_list))\n",
    "\n",
    "print(b)\n",
    "print(b1)\n",
    "\n",
    "move = b1\n",
    "predict_move1(move)\n",
    "probing_legal_moves1(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/sagemaker-user/anlp/prob1/lichess_16layers_ckpt_with_optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model2, input_size=(50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_name = \"lichess_16layers_ckpt_with_optimizer.pt\"\n",
    "checkpoint = torch.load(model_name, map_location='cpu')\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with open(f'lichess_16layers_ckpt_with_optimizer.pt', 'rb') as f:\n",
    "    state_dict = torch.load(f, map_location=torch.device('cpu'))\n",
    "    print(state_dict.keys())\n",
    "    for key in state_dict.keys():\n",
    "        if key != \"model\" and key != \"optimizer\":\n",
    "            print(key, state_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model2, input_size = (1, 12), batch_size = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on 2k games from hf dataset\n",
    "# Ratio legal/total: 66028 / 67621 --> 97.64422294849233l = 0\n",
    "\n",
    "t = 0\n",
    "g1 = 1\n",
    "for g in range(0,1000):\n",
    "    b = chop_game_before_final_moves(ds['train']['text'][g][1:])\n",
    "    b1 = io.StringIO(b)\n",
    "    game = pgn.read_game(b1)\n",
    "    board = game.board()\n",
    "\n",
    "    move_list = []\n",
    "    for move in game.mainline_moves():\n",
    "        move_list.append(board.uci(move))\n",
    "        board.push(move)\n",
    "    b1 = (\" \".join(move_list))\n",
    "\n",
    "    move = b1\n",
    "    #predict_move1(move)\n",
    "    legal, total = probing_legal_moves1(move)    \n",
    "    l = l + legal\n",
    "    t = t + total\n",
    "    print(f\"{g1} / {2000} ------- {legal} / {total} ---> Ratio legal/total: {l} / {t} --> {int(l/t*100)}%\")\n",
    "    g1 += 1\n",
    "\n",
    "print(f\"Ratio legal/total: {l} / {t} --> {(l/t)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pgn_file(file_path):\n",
    "    games = []\n",
    "    with open(file_path, \"r\") as pgn_file:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn_file)\n",
    "            if game is None:\n",
    "                break\n",
    "            games.append(game[0])\n",
    "    return games\n",
    "\n",
    "# Example usage\n",
    "file_path = \"/home/sagemaker-user/anlp/prob1/lichess_Mack9_2024-07-30.pgn\"\n",
    "games = read_pgn_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on my games on lichess (100% they are not on training set)\n",
    "# still around 97% legal moves ratio\n",
    "\n",
    "l = 0\n",
    "t = 0\n",
    "g1 = 1\n",
    "for g in range(len(games)):\n",
    "    b = chop_game_before_final_moves(str(games[g]))\n",
    "    b1 = io.StringIO(b)\n",
    "    game = pgn.read_game(b1)\n",
    "    board = game.board()\n",
    "\n",
    "    move_list = []\n",
    "    for move in game.mainline_moves():\n",
    "        move_list.append(board.uci(move))\n",
    "        board.push(move)\n",
    "    b1 = (\" \".join(move_list))\n",
    "\n",
    "    move = b1\n",
    "    #predict_move1(move)\n",
    "    legal, total = probing_legal_moves1(move)    \n",
    "    l = l + legal\n",
    "    t = t + total\n",
    "    print(f\"{g1} / {2000} ------- {legal} / {total} ---> Ratio legal/total: {l} / {t} --> {int(l/t*100)}%\")\n",
    "    g1 += 1\n",
    "\n",
    "print(f\"Ratio legal/total: {l} / {t} --> {(l/t)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention: Heatmap Probing\n",
    "\n",
    "import chess\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import GPT2LMHeadModel, AutoModel\n",
    "sys.path.append(\"src\")\n",
    "from data_utils.chess_tokenizer import ChessTokenizer\n",
    "import chess.pgn as pgn\n",
    "import io\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import re\n",
    "\n",
    "\n",
    "# Initialize the Model and the Tokenizer\n",
    "vocab_path = \"vocab.txt\"\n",
    "tokenizer = ChessTokenizer(vocab_path)\n",
    "model = GPT2LMHeadModel.from_pretrained('shtoshni/gpt2-chess-uci')\n",
    "\n",
    "num_heads = model.config.num_attention_heads\n",
    "num_layers = len(model.transformer.h)\n",
    "print(f\"Number of attention layers: {num_layers}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "\n",
    "ds = load_dataset(\"adamkarvonen/chess_sae_individual_games_filtered\")\n",
    "\n",
    "def chop_game_at_random_point(pgn):\n",
    "    # Split the PGN string into individual moves\n",
    "    moves = pgn.split()\n",
    "    # Filter out the move numbers (e.g., \"1.\", \"2.\", etc.)\n",
    "    game_moves = [move for move in moves if not move[0].isdigit()]\n",
    "    # Determine the total number of moves\n",
    "    total_moves = len(game_moves)\n",
    "    # Ensure we have an even number of moves for simplicity\n",
    "    if total_moves % 2 != 0:\n",
    "        total_moves -= 1\n",
    "    # Randomly choose a point to chop, ensuring 50/50 white/black split\n",
    "    random_index = random.randint(7, total_moves - 1)\n",
    "    # Determine if the random point should be a white or black move\n",
    "    if random_index % 2 == 0:\n",
    "        chopped_game = ' '.join(moves[:random_index + 1])\n",
    "    else:\n",
    "        chopped_game = ' '.join(moves[:random_index + 2])\n",
    "    return chopped_game\n",
    "\n",
    "def get_model_output(input_sequence):\n",
    "    # Encode the input sequence\n",
    "    encoded = tokenizer.encode(input_sequence, add_special_tokens=False, get_move_end_positions=False)\n",
    "    # Convert to tensor and add batch dimension\n",
    "    inputs = torch.tensor([encoded]).long()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs, output_attentions=True, output_hidden_states=True)\n",
    "    return outputs\n",
    "\n",
    "def visualize_attention0(input_sequence, layer=-1, head=0):\n",
    "    outputs = get_model_output(input_sequence)\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "\n",
    "    decoded_tokens = [tokenizer.decode_token(t) for t in tokenizer.encode(input_sequence, add_special_tokens=False, get_move_end_positions=False)]\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    #sns.heatmap(attention, cmap=\"YlOrRd\")   #--> this for token number\n",
    "    sns.heatmap(attention, cmap=\"YlOrRd\", xticklabels=decoded_tokens, yticklabels=decoded_tokens)\n",
    "    plt.title(f\"Attention weights for layer {layer}, head {head}\")\n",
    "    plt.xlabel(\"Key tokens\")\n",
    "    plt.ylabel(\"Query tokens\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def group_tokens(tokens):\n",
    "    \"\"\"Group tokens in pairs.\"\"\"\n",
    "    grouped_tokens = []\n",
    "    for i in range(0, len(tokens), 2):\n",
    "        if i + 1 < len(tokens):\n",
    "            grouped_tokens.append(f\"{tokens[i]}-{tokens[i + 1]}\")\n",
    "        else:\n",
    "            grouped_tokens.append(tokens[i])  # Handle the case of an odd number of tokens\n",
    "    return grouped_tokens\n",
    "\n",
    "def aggregate_attention(attention, group_size):\n",
    "    \"\"\"Aggregate attention weights by summing over grouped tokens.\"\"\"\n",
    "    num_tokens = attention.shape[0]\n",
    "    num_groups = (num_tokens + group_size - 1) // group_size  # Calculate number of groups\n",
    "\n",
    "    aggregated_attention = np.zeros((num_groups, num_groups))\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        for j in range(num_groups):\n",
    "            start_i, end_i = i * group_size, min((i + 1) * group_size, num_tokens)\n",
    "            start_j, end_j = j * group_size, min((j + 1) * group_size, num_tokens)\n",
    "            aggregated_attention[i, j] = np.sum(attention[start_i:end_i, start_j:end_j])\n",
    "    \n",
    "    return aggregated_attention\n",
    "\n",
    "#from bertviz import head_view, model_view\n",
    "#head_view(attention, input_sequence, tokenizer)\n",
    "\n",
    "def visualize_attention(input_sequence, layer=-1, head=0):\n",
    "    outputs = get_model_output(input_sequence)\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "\n",
    "    decoded_tokens = [tokenizer.decode_token(t) for t in tokenizer.encode(input_sequence, add_special_tokens=False, get_move_end_positions=False)]\n",
    "    grouped_tokens = group_tokens(decoded_tokens)\n",
    "\n",
    "    # comment to show UCI notation, leave to show SAN move (UCI = c8-f5, SAN = Nf5)\n",
    "    grouped_tokens = re.sub(r'\\d+\\.', '', str(game.mainline_moves())).split()\n",
    "\n",
    "    grouped_attention = aggregate_attention(attention, group_size=2)  # Group size of 2 for pairs\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(grouped_attention, cmap=\"YlOrRd\", xticklabels=grouped_tokens, yticklabels=grouped_tokens)\n",
    "    plt.title(f\"Attention weights for layer {layer}, head {head}\")\n",
    "    plt.xlabel(\"Key Moves (2 tokens)\")\n",
    "    plt.ylabel(\"Query Moves (2 tokens)\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def probe_attention(move_sequence, query_key = 'moves'):\n",
    "    moves = move_sequence.split()\n",
    "    if query_key == 'moves':\n",
    "        vis_att = visualize_attention\n",
    "    else:\n",
    "        vis_att = visualize_attention0\n",
    "\n",
    "    if len(moves) < 3:\n",
    "        print(\"The move sequence is too short for meaningful analysis.\")\n",
    "        return\n",
    "\n",
    "    # Visualize attention for the whole sequence\n",
    "    print(\"Visualizing attention for the entire move sequence:\")\n",
    "    num_layers = 2\n",
    "    for i in range(num_layers):\n",
    "        vis_att(move_sequence, i, 1)\n",
    "        vis_att(move_sequence, i, 2)\n",
    "        vis_att(move_sequence, i, 3)\n",
    "        vis_att(move_sequence, i, 4)\n",
    "\n",
    "\n",
    "# Usage\n",
    "g = random.randint(0, 90000)\n",
    "a = ds['train']['text'][g][1:]\n",
    "b = chop_game_at_random_point(a)\n",
    "\n",
    "b1 = io.StringIO(b)\n",
    "game = pgn.read_game(b1)\n",
    "board = game.board()\n",
    "\n",
    "move_list = []\n",
    "for move in game.mainline_moves():\n",
    "    move_list.append(board.uci(move))\n",
    "    board.push(move)\n",
    "\n",
    "b1 = \" \".join(move_list)\n",
    "\n",
    "print(\"Original game:\")\n",
    "print(b)\n",
    "print(\"UCI move sequence:\")\n",
    "print(b1)\n",
    "\n",
    "# Perform probe_attention\n",
    "'''\n",
    "What the heatmaps show:\n",
    "These heatmaps visualize the attention weights in the transformer model (GPT-2 in this case) for a given input sequence of chess moves.\n",
    "Axes of the heatmap:\n",
    "- The y-axis represents the \"query\" tokens (the current token being processed)\n",
    "- The x-axis represents the \"key\" tokens (the tokens being attended to)\n",
    "\n",
    "Color intensity:\n",
    "- Brighter colors (more yellow/red) indicate higher attention weights\n",
    "- Darker colors (more purple/black) indicate lower attention weights\n",
    "\n",
    "Interpretation:\n",
    "- Each cell in the heatmap shows how much attention a particular token (y-axis) pays to another token (x-axis) when making its prediction\n",
    "- The diagonal often shows high attention as tokens frequently attend to themselves\n",
    "\n",
    "What to look for:\n",
    "- Patterns of attention (e.g., attention to recent moves, or to specific types of pieces)\n",
    "- Changes in attention patterns across different layers\n",
    "- Any unexpected or interesting focus of attention that might give insights into how the model is processing the chess game\n",
    "'''\n",
    "\n",
    "'''\n",
    "ATTENTION:\n",
    "- 'head' refers to one of the multiple attention mechanisms (attention heads) used in a multi-head self-attention layer of the transformer \n",
    "model. Each attention head independently focuses on different parts of the input sequence to capture various relationships and features. \n",
    "When visualizing attention, the head parameter allows you to select which of these attention mechanisms to display.7\n",
    "- 'layers', gpt2_small has 12 attention layers.\n",
    "    - First Layer: Choose this if you are interested in seeing how the model begins to process the input, \n",
    "        especially for analyzing low-level, syntactic patterns and initial token interactions.\n",
    "    - Last Layer: Choose this if you want to understand the final, high-level interpretations and \n",
    "        decisions made by the model, especially for tasks that rely on deeper understanding and context\n",
    "'''\n",
    "\n",
    "'''\n",
    "If the heatmap shows a strong red color in the cell that corresponds to token 7 on the y-axis and token 2 on the x-axis, \n",
    "it means that when processing token 7, the model is placing a high attention weight on token 2. \n",
    "In other words, the information from token 2 is heavily influencing the processing of token 7.\n",
    "'''\n",
    "\n",
    "probe_attention(b1, query_key = 'moves')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
